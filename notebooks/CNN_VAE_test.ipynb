{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of CNN-VAE Reconstruction Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join\n",
    "parent_dir = os.path.abspath(join(os.getcwd(), os.pardir))\n",
    "app_dir = join(parent_dir, \"app\")\n",
    "if app_dir not in sys.path:\n",
    "      sys.path.append(app_dir)\n",
    "\n",
    "from pathlib import Path\n",
    "import torch as pt\n",
    "from torch.utils.data import Subset\n",
    "from torch.nn.functional import mse_loss\n",
    "from utils.CNN_VAE import ConvEncoder, ConvDecoder, Autoencoder\n",
    "import utils.config as config\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import numpy as np\n",
    "\n",
    "importlib.reload(config)\n",
    "\n",
    "pt.manual_seed(711)\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 180\n",
    "\n",
    "# use GPU if possible\n",
    "device = pt.device(\"cuda\") if pt.cuda.is_available() else pt.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "TIMESTEP = (config.mini_test_per_cond - 1) if config.mini_dataset else config.timestep_reconstruction\n",
    "\n",
    "DATA_PATH = join(Path(os.path.abspath('')).parent, \"data\", \"VAE\")\n",
    "OUTPUT_PATH = join(Path(os.path.abspath('')).parent, \"output\", \"VAE\")\n",
    "MODEL_PATH = join(Path(os.path.abspath('')).parent, \"output\", \"VAE\", \"latent_study\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate study results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot\n",
    "study_results = pt.load(join(MODEL_PATH, \"study_results.pt\"))\n",
    "latent_sizes = list(study_results.keys())\n",
    "print(latent_sizes)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=config.standard_figsize_1)\n",
    "\n",
    "test_losses = []\n",
    "for i, key in enumerate(latent_sizes):\n",
    "    test_losses.append(np.asarray(\n",
    "        [res[\"test_loss\"].values[-10:].mean() for res in study_results[key]]\n",
    "    ))\n",
    "    ax.boxplot(test_losses[i], positions=[i], flierprops={\n",
    "                \"markersize\": 6, \"markeredgecolor\": \"C3\"})\n",
    "plt.gca().set_xticklabels((\"8\", \"16\", \"64\", \"128\", \"256\", \"512\"))\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Test MSE\")\n",
    "plt.xlabel(\"Number of bottleneck neurons\")\n",
    "plt.gca().grid(True, ls=\"--\",which='both')\n",
    "plt.tight_layout()\n",
    "plt.savefig(join(OUTPUT_PATH, \"VAE_training_tendency_and_spread.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of test loss for each epoch for each latent size\n",
    "means = {}\n",
    "stds = {}\n",
    "\n",
    "for size in study_results:\n",
    "    all_test_losses = []\n",
    "    for df in study_results[size]:\n",
    "        all_test_losses.append(df[\"test_loss\"])\n",
    "    \n",
    "    # Pad shorter training sequences with NaNs to ensure equal lengths\n",
    "    all_test_losses_padded = np.array([np.pad(loss, (0, config.VAE_epochs - len(loss)), mode='constant', constant_values=np.nan) for loss in all_test_losses])\n",
    "    \n",
    "    # Calculate mean and std while ignoring NaN values\n",
    "    means[size] = np.nanmean(all_test_losses_padded, axis=0)\n",
    "    stds[size] = np.nanstd(all_test_losses_padded, axis=0)\n",
    "\n",
    "# Calculate 2 sigma confidence interval for each latent size\n",
    "conf_intervals = {size: (means[size] - 1 * stds[size], means[size] + 1 * stds[size]) for size in study_results}\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=config.standard_figsize_1)\n",
    "\n",
    "# Plot mean test loss with 2 sigma confidence interval for each latent size\n",
    "for size in study_results:\n",
    "    ax.plot(np.arange(1, config.VAE_epochs + 1), means[size], label=f'Latent Size {size}')\n",
    "    #ax.fill_between(np.arange(1, config.epochs + 1), conf_intervals[size][0], conf_intervals[size][1], alpha=0.3)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xlim(0, config.VAE_epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Test MSE Mean\")\n",
    "plt.legend(loc=1, bbox_to_anchor=(1,1))\n",
    "plt.tight_layout()\n",
    "plt.savefig(join(OUTPUT_PATH, \"VAE_test_loss_mean.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best performing model for each latent size based on the mean of the last 10 test loss values\n",
    "best_models = {str(latent_size): np.argmin(test_losses[i]) + 1 for i, latent_size in enumerate(latent_sizes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_results = pt.load(join(MODEL_PATH, \"study_results.pt\"))\n",
    "# latent_sizes = list(study_results.keys())\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "# means, stds = [], []\n",
    "# for i, key in enumerate(latent_sizes):\n",
    "#     test_loss = np.asarray(\n",
    "#         [res[\"test_loss\"].values[-10:].mean() for res in study_results[key]]\n",
    "#     )\n",
    "#     ax1.boxplot(test_loss, positions=[i], flierprops={\n",
    "#                 \"markersize\": 6, \"markeredgecolor\": \"C3\"})\n",
    "    \n",
    "#     means.append(np.mean(test_loss))\n",
    "#     stds.append(np.std(test_loss))\n",
    "\n",
    "\n",
    "# conf_intervals = [(mean - 2 * std, mean + 2 * std) for mean, std in zip(means, stds)]\n",
    "# ax2.errorbar(range(len(latent_sizes)), means, yerr=2 * np.array(stds), fmt='o', capsize=5)\n",
    "    \n",
    "# ax1.set_yscale(\"log\")\n",
    "# ax1.set_ylabel(\"Test MSE\")\n",
    "# ax2.set_title('Mean and Variance with 2\\u03C3 Confidence Interval')\n",
    "\n",
    "# ax2.set_yscale(\"log\")\n",
    "# ax2.set_ylabel(\"Test MSE\")\n",
    "# ax2.set_title('Mean and Variance with 2\\u03C3 Confidence Interval')\n",
    "# ax2.set_xticklabels(latent_sizes)\n",
    "# ax2.set_xlabel(\"Number of bottleneck neurons\")\n",
    "# plt.gca().grid(ls=\"--\")\n",
    "# fig.tight_layout\n",
    "# fig.savefig(join(OUTPUT_PATH, \"VAE_training_tendency_and_spread.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "test_dataset = pt.load(join(DATA_PATH, \"test_dataset.pt\"))\n",
    "\n",
    "# split test dataset into the two flow conditions\n",
    "X_test_1 = Subset(test_dataset,                                 # ma0.84 alpha3.00 \n",
    "                  list(range(0, int(len(test_dataset) / 2))))        \n",
    "X_test_2 = Subset(test_dataset,                                 # ma0.84 alpha5.00\n",
    "                  list(range(int(len(test_dataset) / 2), len(test_dataset))))    \n",
    "\n",
    "# make tensors from datasets\n",
    "X_test_1_tensor = pt.stack([X_test_1[n] for n in range(len(X_test_1))], dim=3).squeeze(0)\n",
    "X_test_2_tensor = pt.stack([X_test_2[n] for n in range(len(X_test_2))], dim=3).squeeze(0)\n",
    "print(X_test_1_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE and Variance Reconstruction with varying number of bottleneck neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create VAE model\n",
    "def make_VAE_model(n_latent: int) -> pt.nn.Module:\n",
    "    encoder = ConvEncoder(\n",
    "        in_size=config.target_resolution,\n",
    "        n_channels=config.input_channels,\n",
    "        n_latent=n_latent,\n",
    "        variational=True,\n",
    "        layernorm=True\n",
    "    )\n",
    "\n",
    "    decoder = ConvDecoder(\n",
    "        in_size=config.target_resolution,\n",
    "        n_channels=config.output_channels,\n",
    "        n_latent=n_latent,\n",
    "        layernorm=True,\n",
    "        squash_output=True\n",
    "    )\n",
    "\n",
    "    autoencoder = Autoencoder(encoder, decoder)\n",
    "    autoencoder.to(device)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan directory for trained models and extract paths as well as the latent size of the best performing model\n",
    "dirs = [os.path.join(MODEL_PATH, name, str(best_models[name]) + \"_\" + name) for name in os.listdir(MODEL_PATH) if os.path.isdir(os.path.join(MODEL_PATH, name))]\n",
    "sorted_dirs = sorted(dirs, key=lambda x: int(os.path.basename(x).split('_')[1]))\n",
    "print(sorted_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize lists to save the computed metrics\n",
    "# MSE_1 = []\n",
    "# MSE_2 = []\n",
    "# Var1 = []\n",
    "# Var2 = []\n",
    "\n",
    "# # compute the total variance of test datasets\n",
    "# orig_Var1 = pt.var(X_test_1_tensor)\n",
    "# orig_Var2 = pt.var(X_test_2_tensor)\n",
    "\n",
    "# for i, latent_size in enumerate(latent_sizes):\n",
    "#     print(\"Computing metrics for autoencoder with latent size \", latent_size)\n",
    "#     # load model\n",
    "#     autoencoder = make_VAE_model(latent_size)\n",
    "#     autoencoder.load(sorted_dirs[i])\n",
    "#     autoencoder.eval()\n",
    "\n",
    "#     # reconstruct test dataset 1\n",
    "#     with pt.no_grad():\n",
    "#         reconstructed = pt.stack([autoencoder(X_test_1[n].unsqueeze(0)).squeeze(0).detach() for n in range(len(X_test_1))], dim=3).squeeze(0)\n",
    "    \n",
    "#     # compute MSE\n",
    "#     MSE_1.append(mse_loss(X_test_1_tensor, reconstructed).item())\n",
    "\n",
    "#     # compute variance reconstruction\n",
    "#     Var1.append(((1 - ((orig_Var1 - pt.var(reconstructed)) / orig_Var1)) * 100).item())\n",
    "\n",
    "#     # reconstruct test dataset 2\n",
    "#     with pt.no_grad():\n",
    "#         reconstructed = pt.stack([autoencoder(X_test_2[n].unsqueeze(0)).squeeze(0).detach() for n in range(len(X_test_2))], dim=3).squeeze(0)\n",
    "\n",
    "#     # compute MSE\n",
    "#     MSE_2.append(mse_loss(X_test_2_tensor, reconstructed).item())\n",
    "\n",
    "#     # compute variance reconstruction\n",
    "#     Var2.append(((1 - ((orig_Var2 - pt.var(reconstructed)) / orig_Var2)) * 100).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the results and save the figure\n",
    "# fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "# ax1.plot(latent_sizes, MSE_1, label=\"Test Dataset 1\")\n",
    "# ax1.plot(latent_sizes, MSE_2, label=\"Test Dataset 2\")\n",
    "# ax1.set_title(\"MSE\")\n",
    "# ax2.plot(latent_sizes, Var1, label=\"Test Dataset 1\")\n",
    "# ax2.plot(latent_sizes, Var2, label=\"Test Dataset 2\")\n",
    "# ax2.set_title(\"Variance Reconstruction in %\")\n",
    "# ax2.set_xlabel(\"latent size\")\n",
    "# ax2.set_xticks(range(0, 501, 50))\n",
    "# handles, labels = ax2.get_legend_handles_labels()\n",
    "# fig.legend(handles, labels)\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(join(OUTPUT_PATH, \"MSE_and_Variance_with_latent_size.png\"), bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal MSE distribution with varying number of bottleneck neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = range(config.mini_test_per_cond) if config.mini_dataset else range(config.time_steps_per_cond)\n",
    "\n",
    "fig, ax1 = plt.subplots(1, 1, figsize = (8, 3))\n",
    "    \n",
    "for i, latent_size in enumerate(latent_sizes):\n",
    "    print(\"Computing metrics for autoencoder with latent size \", latent_size)\n",
    "    # load model\n",
    "    autoencoder = make_VAE_model(int(latent_size))\n",
    "    autoencoder.load(sorted_dirs[i])\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # reconstruct test dataset 1\n",
    "    with pt.no_grad():\n",
    "        reconstructed = pt.stack([autoencoder(X_test_1[n].unsqueeze(0)).squeeze(0).detach() for n in range(len(X_test_1))], dim=3).squeeze(0)\n",
    "\n",
    "    MSE = ((X_test_1_tensor - reconstructed)**2).mean(dim=[0, 1])\n",
    "    ax1.plot(timesteps, MSE, label=\"latent size {}\".format(latent_size))\n",
    "\n",
    "# ax1.set_title(\"Test Dataset 1\")\n",
    "ax1.set_ylabel(\"MSE\")\n",
    "ax1.set_xlabel(\"timestep\")\n",
    "# ax1.set_yscale(\"log\")\n",
    "\n",
    "fig.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(join(OUTPUT_PATH, \"VAE_temporal_MSE_distribution.png\"), bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial MSE distribution with varying number of bottleneck neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coordinates\n",
    "coords = pt.load(join(DATA_PATH, \"coords_interp.pt\"))\n",
    "xx, yy = coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, sharey=True)\n",
    "\n",
    "for i, latent_size in enumerate([64, 128, 256]):\n",
    "    print(\"Computing metrics for autoencoder with latent size \", latent_size)\n",
    "    # load model\n",
    "    autoencoder = make_VAE_model(latent_size)\n",
    "    autoencoder.load(sorted_dirs[latent_sizes.index(str(latent_size))])\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # reconstruct test dataset 1\n",
    "    with pt.no_grad():\n",
    "        reconstructed = pt.stack([autoencoder(X_test_1[n].unsqueeze(0)).squeeze(0).detach() for n in range(len(X_test_1))], dim=3).squeeze(0)\n",
    "\n",
    "    MSE1 = ((X_test_1_tensor - reconstructed)**2).mean(dim=2)\n",
    "\n",
    "    # reconstruct test dataset 2\n",
    "    with pt.no_grad():\n",
    "        reconstructed = pt.stack([autoencoder(X_test_2[n].unsqueeze(0)).squeeze(0).detach() for n in range(len(X_test_2))], dim=3).squeeze(0)\n",
    "\n",
    "    MSE2 = ((X_test_2_tensor - reconstructed)**2).mean(dim=2)\n",
    "\n",
    "    # compute the mean MSE and the corresponding standard deviation for the lowest rank to compare to higher ranks\n",
    "    if i == 0:\n",
    "        mean, std = MSE1.mean(), MSE1.std()\n",
    "        vmin, vmax = 0, mean + 1*std\n",
    "        levels = pt.linspace(vmin, vmax, 120)\n",
    "\n",
    "        axes[0][i].set_ylabel(\"Test Dataset 1\")\n",
    "        axes[1][i].set_ylabel(\"Test Dataset 2\")\n",
    "\n",
    "    # create the contour plot\n",
    "    cont = axes[0][i].contourf(xx, yy, MSE1, vmin=vmin, vmax=vmax, levels=levels, extend=\"both\")\n",
    "    cont = axes[1][i].contourf(xx, yy, MSE2, vmin=vmin, vmax=vmax, levels=levels, extend=\"both\")\n",
    "\n",
    "    # formatting\n",
    "    axes[0][i].set_title(\"latent size {}\".format(latent_size))\n",
    "\n",
    "    for row in range(2):\n",
    "        axes[row][i].set_aspect(\"equal\")\n",
    "        axes[row][i].set_xticklabels([])\n",
    "        axes[row][i].set_yticklabels([])\n",
    "\n",
    "# add seperate subplot for color axis\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cax = fig.add_axes([0.99, 0.042, 0.03, 0.885])\n",
    "cbar = fig.colorbar(cont, cax=cax,label = \"MSE\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(join(OUTPUT_PATH, \"VAE_spatial_MSE_distribution.png\"), bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstructed pressure field compared to Ground Truth for two bottleneck sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, sharey=True)\n",
    "\n",
    "for i, latent_size in enumerate([64, 128, \"experimental\"]):\n",
    "    # create the contour plot\n",
    "    if latent_size == \"experimental\":\n",
    "        cont = axes[0][i].contourf(xx, yy, X_test_1[TIMESTEP].squeeze(0), vmin=vmin, vmax=vmax, levels=levels, extend=\"both\")\n",
    "        cont = axes[1][i].contourf(xx, yy, X_test_2[TIMESTEP].squeeze(0), vmin=vmin, vmax=vmax, levels=levels, extend=\"both\")\n",
    "        axes[0][i].set_title(\"Ground Truth\")\n",
    "    else:\n",
    "        # load model\n",
    "        autoencoder = make_VAE_model(latent_size)\n",
    "        autoencoder.load(sorted_dirs[latent_sizes.index(str(latent_size))])\n",
    "        autoencoder.eval()\n",
    "\n",
    "        # reconstruct test dataset 1\n",
    "        with pt.no_grad():\n",
    "            reconstructed_timestep1 = autoencoder(X_test_1[TIMESTEP].unsqueeze(0)).detach().squeeze()\n",
    "\n",
    "        # reconstruct test dataset 2\n",
    "        with pt.no_grad():\n",
    "            reconstructed_timestep2 = autoencoder(X_test_2[TIMESTEP].unsqueeze(0)).detach().squeeze()\n",
    "\n",
    "        # compute the mean MSE and the corresponding standard deviation for the lowest rank to compare to higher ranks\n",
    "        if i == 0:\n",
    "            mean, std = reconstructed_timestep1.mean(), reconstructed_timestep1.std()\n",
    "            vmin, vmax = mean - 2*std, mean + 2*std\n",
    "            levels = pt.linspace(vmin, vmax, 120)\n",
    "\n",
    "            axes[0][i].set_ylabel(\"Test Dataset 1\")\n",
    "            axes[1][i].set_ylabel(\"Test Dataset 2\")\n",
    "\n",
    "        cont = axes[0][i].contourf(xx, yy, reconstructed_timestep1, vmin=vmin, vmax=vmax, levels=levels, extend=\"both\")\n",
    "        cont = axes[1][i].contourf(xx, yy, reconstructed_timestep2, vmin=vmin, vmax=vmax, levels=levels, extend=\"both\")\n",
    "        axes[0][i].set_title(\"latent size {}\".format(latent_size))\n",
    "    \n",
    "    for row in range(2):\n",
    "        axes[row][i].set_aspect(\"equal\")\n",
    "        axes[row][i].set_xticklabels([])\n",
    "        axes[row][i].set_yticklabels([])\n",
    "\n",
    "    # add seperate subplot for color axis\n",
    "    fig.subplots_adjust(right=0.9)\n",
    "    cax = fig.add_axes([0.99, 0.042, 0.03, 0.885])\n",
    "    cbar = fig.colorbar(cont, cax=cax,label = r\"$c_p$\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(join(OUTPUT_PATH, \"VAE_timestep_reconstruction.png\"), bbox_inches = \"tight\")\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
