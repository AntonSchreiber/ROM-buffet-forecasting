{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Reconstruction Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join\n",
    "parent_dir = os.path.abspath(join(os.getcwd(), os.pardir))\n",
    "app_dir = join(parent_dir, \"app\")\n",
    "if app_dir not in sys.path:\n",
    "      sys.path.append(app_dir)\n",
    "\n",
    "from pathlib import Path\n",
    "import torch as pt\n",
    "from torch.utils.data import Subset\n",
    "from torch.nn.functional import mse_loss\n",
    "from CNN_VAE import ConvEncoder, ConvDecoder, Autoencoder\n",
    "from utils.training_loop import train_cnn_vae\n",
    "import utils.config as config\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "importlib.reload(config)\n",
    "\n",
    "pt.manual_seed(0)\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 180\n",
    "\n",
    "# use GPU if possible\n",
    "device = pt.device(\"cuda:0\") if pt.cuda.is_available() else pt.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "TIMESTEP = (config.mini_test_per_cond - 1) if config.mini_dataset else config.timestep_reconstruction\n",
    "\n",
    "DATA_PATH = Path(os.path.abspath('')).parent / \"data\"\n",
    "OUTPUT_PATH = Path(os.path.abspath('')).parent / \"output\" / \"VAE\"\n",
    "MODEL_PATH = Path(os.path.abspath('')).parent / \"output\" / \"VAE\" / \"latent_study\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "test_dataset = pt.load(join(DATA_PATH, \"test_dataset.pt\"))\n",
    "\n",
    "# split test dataset into the two flow conditions\n",
    "X_test_1 = Subset(test_dataset,                                 # ma0.84 alpha3.00 \n",
    "                  list(range(0, int(len(test_dataset) / 2))))        \n",
    "X_test_2 = Subset(test_dataset,                                 # ma0.84 alpha5.00\n",
    "                  list(range(int(len(test_dataset) / 2), len(test_dataset))))    \n",
    "\n",
    "# make tensors from datasets\n",
    "X_test_1_tensor = pt.stack([X_test_1[n] for n in range(len(X_test_1))], dim=3).squeeze(0)\n",
    "X_test_2_tensor = pt.stack([X_test_2[n] for n in range(len(X_test_2))], dim=3).squeeze(0)\n",
    "print(X_test_1_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE and Variance Reconstruction with varying number of bottleneck neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create VAE model\n",
    "def make_VAE_model(n_latent: int = 256) -> pt.nn.Module:\n",
    "    encoder = ConvEncoder(\n",
    "        in_size=config.target_resolution,\n",
    "        n_channels=config.input_channels,\n",
    "        n_latent=n_latent,\n",
    "        variational=True,\n",
    "        layernorm=True\n",
    "    )\n",
    "\n",
    "    decoder = ConvDecoder(\n",
    "        in_size=config.target_resolution,\n",
    "        n_channels=config.output_channels,\n",
    "        n_latent=n_latent,\n",
    "        layernorm=True,\n",
    "        squash_output=True\n",
    "    )\n",
    "\n",
    "    autoencoder = Autoencoder(encoder, decoder)\n",
    "    autoencoder.to(device)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scan directory for trained models and extract paths as well as the latent size of the model\n",
    "dirs = [os.path.join(MODEL_PATH, name, name) for name in os.listdir(MODEL_PATH) if os.path.isdir(os.path.join(MODEL_PATH, name))]\n",
    "sorted_dirs = sorted(dirs, key=lambda x: int(os.path.basename(x)))\n",
    "latent_sizes = [int(os.path.basename(dir)) for dir in sorted_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to save the computed metrics\n",
    "MSE_1 = []\n",
    "MSE_2 = []\n",
    "Var1 = []\n",
    "Var2 = []\n",
    "\n",
    "# compute the total variance of test datasets\n",
    "orig_Var1 = pt.var(X_test_1_tensor)\n",
    "orig_Var2 = pt.var(X_test_2_tensor)\n",
    "\n",
    "for i, latent_size in enumerate(latent_sizes):\n",
    "    print(\"Computing metrics for autoencoder with latent size \", latent_size)\n",
    "    # load model\n",
    "    autoencoder = make_VAE_model(latent_size)\n",
    "    autoencoder.load(sorted_dirs[i])\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # reconstruct test dataset 1\n",
    "    with pt.no_grad():\n",
    "        reconstructed = pt.stack([autoencoder(X_test_1[n].unsqueeze(0)).squeeze(0).detach() for n in range(len(X_test_1))], dim=3).squeeze(0)\n",
    "    \n",
    "    print(reconstructed)\n",
    "    # compute MSE\n",
    "    MSE_1.append(mse_loss(X_test_1_tensor, reconstructed).item())\n",
    "\n",
    "    # compute variance reconstruction\n",
    "    Var1.append(((1 - ((orig_Var1 - pt.var(reconstructed)) / orig_Var1)) * 100).item())\n",
    "\n",
    "    # reconstruct test dataset 2\n",
    "    with pt.no_grad():\n",
    "        reconstructed = pt.stack([autoencoder(X_test_2[n].unsqueeze(0)).squeeze(0).detach() for n in range(len(X_test_2))], dim=3).squeeze(0)\n",
    "\n",
    "    # compute MSE\n",
    "    MSE_2.append(mse_loss(X_test_2_tensor, reconstructed).item())\n",
    "\n",
    "    # compute variance reconstruction\n",
    "    Var2.append(((1 - ((orig_Var2 - pt.var(reconstructed)) / orig_Var2)) * 100).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results and save the figure\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "ax1.plot(latent_sizes, MSE_1, label=\"Test Dataset 1\")\n",
    "ax1.plot(latent_sizes, MSE_2, label=\"Test Dataset 2\")\n",
    "ax1.set_title(\"MSE\")\n",
    "ax2.plot(latent_sizes, Var1, label=\"Test Dataset 1\")\n",
    "ax2.plot(latent_sizes, Var2, label=\"Test Dataset 2\")\n",
    "ax2.set_title(\"Variance Reconstruction in %\")\n",
    "ax2.set_xlabel(\"latent size\")\n",
    "ax2.set_xticks(range(0, 325, 25))\n",
    "handles, labels = ax2.get_legend_handles_labels()\n",
    "fig.legend(handles, labels)\n",
    "fig.tight_layout()\n",
    "fig.savefig(join(OUTPUT_PATH, \"MSE_and_Variance_with_latent_size.png\"), bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal MSE distribution with varying number of bottleneck neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = range(config.mini_test_per_cond) if config.mini_dataset else range(config.time_steps_per_cond)\n",
    "\n",
    "fig, ax1 = plt.subplots(1, 1, figsize = (8, 3))\n",
    "    \n",
    "for i, latent_size in enumerate(latent_sizes):\n",
    "    print(\"Computing metrics for autoencoder with latent size \", latent_size)\n",
    "    # load model\n",
    "    autoencoder = make_VAE_model(latent_size)\n",
    "    autoencoder.load(sorted_dirs[i])\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # reconstruct test dataset 1\n",
    "    with pt.no_grad():\n",
    "        reconstructed = pt.stack([autoencoder(X_test_1[n].unsqueeze(0)).squeeze(0).detach() for n in range(len(X_test_1))], dim=3).squeeze(0)\n",
    "\n",
    "    MSE = ((X_test_1_tensor - reconstructed)**2).mean(dim=[0, 1])\n",
    "    ax1.plot(timesteps, MSE, label=\"latent size {}\".format(latent_size))\n",
    "\n",
    "# ax1.set_title(\"Test Dataset 1\")\n",
    "ax1.set_ylabel(\"MSE\")\n",
    "ax1.set_xlabel(\"timestep\")\n",
    "# ax1.set_yscale(\"log\")\n",
    "\n",
    "fig.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(join(OUTPUT_PATH, \"temporal_MSE_distribution_VAE.png\"), bbox_inches = \"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial MSE distribution with varying number of bottleneck neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coordinates\n",
    "coords = pt.load(join(DATA_PATH, \"coords_interp.pt\"))\n",
    "xx, yy = coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, sharey=True)\n",
    "\n",
    "for i, latent_size in enumerate([10, 110, 210]):\n",
    "    print(\"Computing metrics for autoencoder with latent size \", latent_size)\n",
    "    # load model\n",
    "    autoencoder = make_VAE_model(latent_size)\n",
    "    autoencoder.load(sorted_dirs[latent_sizes.index(latent_size)])\n",
    "    autoencoder.eval()\n",
    "\n",
    "    # reconstruct test dataset 1\n",
    "    with pt.no_grad():\n",
    "        reconstructed = pt.stack([autoencoder(X_test_1[n].unsqueeze(0)).squeeze(0).detach() for n in range(len(X_test_1))], dim=3).squeeze(0)\n",
    "\n",
    "    MSE1 = ((X_test_1_tensor - reconstructed)**2).mean(dim=2)\n",
    "\n",
    "    # reconstruct test dataset 2\n",
    "    with pt.no_grad():\n",
    "        reconstructed = pt.stack([autoencoder(X_test_2[n].unsqueeze(0)).squeeze(0).detach() for n in range(len(X_test_2))], dim=3).squeeze(0)\n",
    "\n",
    "    MSE2 = ((X_test_2_tensor - reconstructed)**2).mean(dim=2)\n",
    "\n",
    "    # compute the mean MSE and the corresponding standard deviation for the lowest rank to compare to higher ranks\n",
    "    if i == 0:\n",
    "        mean, std = MSE1.mean(), MSE1.std()\n",
    "        vmin, vmax = 0, mean + 1*std\n",
    "        levels = pt.linspace(vmin, vmax, 120)\n",
    "\n",
    "        axes[0][i].set_ylabel(\"Test Dataset 1\")\n",
    "        axes[1][i].set_ylabel(\"Test Dataset 2\")\n",
    "\n",
    "    # create the contour plot\n",
    "    cont = axes[0][i].contourf(xx, yy, MSE1, vmin=vmin, vmax=vmax, levels=levels, extend=\"both\")\n",
    "    cont = axes[1][i].contourf(xx, yy, MSE2, vmin=vmin, vmax=vmax, levels=levels, extend=\"both\")\n",
    "\n",
    "    # formatting\n",
    "    axes[0][i].set_title(\"latent size {}\".format(latent_size))\n",
    "\n",
    "    for row in range(2):\n",
    "        axes[row][i].set_aspect(\"equal\")\n",
    "        axes[row][i].set_xticklabels([])\n",
    "        axes[row][i].set_yticklabels([])\n",
    "\n",
    "# add seperate subplot for color axis\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cax = fig.add_axes([0.99, 0.042, 0.03, 0.885])\n",
    "cbar = fig.colorbar(cont, cax=cax,label = \"MSE\")\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(join(OUTPUT_PATH, \"spatial_MSE_distribution_VAE.png\"), bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstructed pressure field compared to Ground Truth for two bottleneck sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, sharey=True)\n",
    "\n",
    "for i, latent_size in enumerate([10, 210, \"experimental\"]):\n",
    "    # create the contour plot\n",
    "    if latent_size == \"experimental\":\n",
    "        cont = axes[0][i].contourf(xx, yy, X_test_1[TIMESTEP].squeeze(0), vmin=vmin, vmax=vmax, levels=levels, extend=\"both\")\n",
    "        cont = axes[1][i].contourf(xx, yy, X_test_2[TIMESTEP].squeeze(0), vmin=vmin, vmax=vmax, levels=levels, extend=\"both\")\n",
    "        axes[0][i].set_title(\"Ground Truth\")\n",
    "    else:\n",
    "        # load model\n",
    "        autoencoder = make_VAE_model(latent_size)\n",
    "        autoencoder.load(sorted_dirs[latent_sizes.index(latent_size)])\n",
    "        autoencoder.eval()\n",
    "\n",
    "        # reconstruct test dataset 1\n",
    "        with pt.no_grad():\n",
    "            reconstructed_timestep1 = autoencoder(X_test_1[TIMESTEP].unsqueeze(0)).detach().squeeze()\n",
    "\n",
    "        # reconstruct test dataset 2\n",
    "        with pt.no_grad():\n",
    "            reconstructed_timestep2 = autoencoder(X_test_2[TIMESTEP].unsqueeze(0)).detach().squeeze()\n",
    "\n",
    "        # compute the mean MSE and the corresponding standard deviation for the lowest rank to compare to higher ranks\n",
    "        if i == 0:\n",
    "            mean, std = reconstructed_timestep1.mean(), reconstructed_timestep1.std()\n",
    "            vmin, vmax = mean - 2*std, mean + 2*std\n",
    "            levels = pt.linspace(vmin, vmax, 120)\n",
    "\n",
    "            axes[0][i].set_ylabel(\"Test Dataset 1\")\n",
    "            axes[1][i].set_ylabel(\"Test Dataset 2\")\n",
    "\n",
    "        cont = axes[0][i].contourf(xx, yy, reconstructed_timestep1, vmin=vmin, vmax=vmax, levels=levels, extend=\"both\")\n",
    "        cont = axes[1][i].contourf(xx, yy, reconstructed_timestep2, vmin=vmin, vmax=vmax, levels=levels, extend=\"both\")\n",
    "        axes[0][i].set_title(\"latent size {}\".format(latent_size))\n",
    "    \n",
    "    for row in range(2):\n",
    "        axes[row][i].set_aspect(\"equal\")\n",
    "        axes[row][i].set_xticklabels([])\n",
    "        axes[row][i].set_yticklabels([])\n",
    "\n",
    "    # add seperate subplot for color axis\n",
    "    fig.subplots_adjust(right=0.9)\n",
    "    cax = fig.add_axes([0.99, 0.042, 0.03, 0.885])\n",
    "    cbar = fig.colorbar(cont, cax=cax,label = r\"$c_p$\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(join(OUTPUT_PATH, \"timestep_reconstruction_VAE.png\"), bbox_inches = \"tight\")\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
