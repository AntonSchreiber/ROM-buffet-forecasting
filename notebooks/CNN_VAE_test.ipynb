{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Reconstruction Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join\n",
    "parent_dir = os.path.abspath(join(os.getcwd(), os.pardir))\n",
    "app_dir = join(parent_dir, \"app\")\n",
    "if app_dir not in sys.path:\n",
    "      sys.path.append(app_dir)\n",
    "\n",
    "from pathlib import Path\n",
    "import torch as pt\n",
    "from torch.utils.data import Subset\n",
    "from torch.nn.functional import mse_loss\n",
    "from CNN_VAE import ConvEncoder, ConvDecoder, Autoencoder\n",
    "from utils.training_loop import train_cnn_vae\n",
    "import utils.config as config\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pt.manual_seed(0)\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 180\n",
    "\n",
    "# use GPU if possible\n",
    "device = pt.device(\"cuda:0\") if pt.cuda.is_available() else pt.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "DATA_PATH = Path(os.path.abspath('')).parent / \"data\"\n",
    "OUTPUT_PATH = Path(os.path.abspath('')).parent / \"output\" / \"VAE\"\n",
    "MODEL_PATH = Path(os.path.abspath('')).parent / \"output\" / \"VAE\" / \"latent_study\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 128, 1])\n"
     ]
    }
   ],
   "source": [
    "# load test dataset\n",
    "test_dataset = pt.load(join(DATA_PATH, \"test_dataset.pt\"))\n",
    "\n",
    "# split test dataset into the two flow conditions\n",
    "X_test_1 = Subset(test_dataset,                                 # ma0.84 alpha3.00 \n",
    "                  list(range(0, int(len(test_dataset) / 2))))        \n",
    "X_test_2 = Subset(test_dataset,                                 # ma0.84 alpha5.00\n",
    "                  list(range(int(len(test_dataset) / 2), len(test_dataset))))    \n",
    "\n",
    "# make tensors from datasets\n",
    "X_test_1_tensor = pt.stack([X_test_1[i] for i in range(len(X_test_1))], dim=3).squeeze(0)\n",
    "X_test_2_tensor = pt.stack([X_test_2[i] for i in range(len(X_test_2))], dim=3).squeeze(0)\n",
    "print(X_test_1_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE and Variance Reconstruction with varying number of bottleneck neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to save the computed metrics\n",
    "MSE_1 = []\n",
    "MSE_2 = []\n",
    "Var1 = []\n",
    "Var2 = []\n",
    "\n",
    "# compute the total variance of test datasets\n",
    "orig_Var1 = pt.var(X_test_1_tensor)\n",
    "orig_Var2 = pt.var(X_test_2_tensor)\n",
    "\n",
    "# scan directory for trained models and extract paths as well as the latent size of the model\n",
    "dirs = [os.path.join(MODEL_PATH, name, name) for name in os.listdir(MODEL_PATH) if os.path.isdir(os.path.join(MODEL_PATH, name))]\n",
    "sorted_dirs = sorted(dirs, key=lambda x: int(os.path.basename(x)))\n",
    "latent_sizes = [int(os.path.basename(dir)) for dir in sorted_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create VAE model\n",
    "def make_VAE_model(n_latent: int = 256) -> pt.nn.Module:\n",
    "    encoder = ConvEncoder(\n",
    "        in_size=config.target_resolution,\n",
    "        n_channels=config.input_channels,\n",
    "        n_latent=config.latent_size,\n",
    "        variational=True,\n",
    "        layernorm=True\n",
    "    )\n",
    "\n",
    "    decoder = ConvDecoder(\n",
    "        in_size=config.target_resolution,\n",
    "        n_channels=config.output_channels,\n",
    "        n_latent=config.latent_size,\n",
    "        layernorm=True,\n",
    "        squash_output=True\n",
    "    )\n",
    "\n",
    "    autoencoder = Autoencoder(encoder, decoder)\n",
    "    autoencoder.to(device)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anton/repositories/Studienarbeit/output/VAE/latent_study/0/0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Sequential:\n\tUnexpected key(s) in state_dict: \"_layers.0.weight\", \"_layers.0.bias\", \"_layers.1.weight\", \"_layers.1.bias\", \"_layers.2.weight\", \"_layers.2.bias\", \"_layers.3.weight\", \"_layers.3.bias\", \"_layers.4.weight\", \"_layers.4.bias\", \"_layers.5.weight\", \"_layers.5.bias\", \"_layers.6.weight\", \"_layers.6.bias\", \"_layers.7.weight\", \"_layers.7.bias\", \"_latent_mean.weight\", \"_latent_mean.bias\", \"_latent_log_var.weight\", \"_latent_log_var.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# load model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m autoencoder \u001b[39m=\u001b[39m Autoencoder()\n\u001b[0;32m----> 5\u001b[0m autoencoder\u001b[39m.\u001b[39;49mload(sorted_dirs[i])\n\u001b[1;32m      7\u001b[0m \u001b[39m# reconstruct test dataset 1 and compute MSE\u001b[39;00m\n\u001b[1;32m      8\u001b[0m reconstructed \u001b[39m=\u001b[39m pt\u001b[39m.\u001b[39mstack([autoencoder(X_test_1[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X_test_1))], dim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/repositories/Studienarbeit/app/CNN_VAE.py:226\u001b[0m, in \u001b[0;36mAutoencoder.load\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m, path: \u001b[39mstr\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 226\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encoder\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(path \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m_encoder.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    227\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoder\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_decoder.pt\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/conda_envs/studienarbeit/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Sequential:\n\tUnexpected key(s) in state_dict: \"_layers.0.weight\", \"_layers.0.bias\", \"_layers.1.weight\", \"_layers.1.bias\", \"_layers.2.weight\", \"_layers.2.bias\", \"_layers.3.weight\", \"_layers.3.bias\", \"_layers.4.weight\", \"_layers.4.bias\", \"_layers.5.weight\", \"_layers.5.bias\", \"_layers.6.weight\", \"_layers.6.bias\", \"_layers.7.weight\", \"_layers.7.bias\", \"_latent_mean.weight\", \"_latent_mean.bias\", \"_latent_log_var.weight\", \"_latent_log_var.bias\". "
     ]
    }
   ],
   "source": [
    "for i, latent_size in enumerate(latent_sizes):\n",
    "    print(sorted_dirs[i])\n",
    "    # load model\n",
    "    autoencoder = make_VAE_model(latent_size)\n",
    "    autoencoder.load(sorted_dirs[i])\n",
    "\n",
    "    # reconstruct test dataset 1 and compute MSE\n",
    "    reconstructed = pt.stack([autoencoder(X_test_1[i]) for i in range(len(X_test_1))], dim=3).squeeze(0)\n",
    "    MSE_1.append(mse_loss(X_test_1_tensor, reconstructed).item())\n",
    "\n",
    "    # compute variance reconstruction\n",
    "    Var1.append((1 - ((orig_Var1 - pt.var(reconstructed)) / orig_Var1)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results and save the figure\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "ax1.plot(latent_sizes, MSE_1, label=\"Test Dataset 1\")\n",
    "ax1.plot(latent_sizes, MSE_2, label=\"Test Dataset 2\")\n",
    "ax1.set_title(\"MSE\")\n",
    "ax2.plot(latent_sizes, Var1, label=\"Test Dataset 1\")\n",
    "ax2.plot(latent_sizes, Var2, label=\"Test Dataset 2\")\n",
    "ax2.set_title(\"Variance Reconstruction in %\")\n",
    "ax2.set_xlabel(\"number of bottleneck neurons\")\n",
    "ax2.set_xticks(range(0, 325, 25))\n",
    "handles, labels = ax2.get_legend_handles_labels()\n",
    "fig.legend(handles, labels)\n",
    "fig.tight_layout()\n",
    "fig.savefig(join(OUTPUT_PATH, \"MSE_and_Variance_with_latent_size.png\"), bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal MSE distribution with varying number of bottleneck neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial MSE distribution with varying number of bottleneck neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstructed pressure field compared to Ground Truth for two bottleneck sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
