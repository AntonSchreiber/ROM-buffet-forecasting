{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# include app directory into sys.path\n",
    "parent_dir = Path(os.path.abspath('')).parent\n",
    "app_dir = join(parent_dir, \"app\")\n",
    "if app_dir not in sys.path:\n",
    "      sys.path.append(app_dir)\n",
    "\n",
    "import torch as pt\n",
    "from torch.nn.functional import mse_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import utils.config as config\n",
    "from utils.helper_funcs import find_target_index_in_dataset\n",
    "from FC.FullyConnected import make_FC_model\n",
    "from utils.helper_funcs import shift_input_sequence, reduce_datasets_SVD, reduce_datasets_VAE\n",
    "from utils.DataWindow import DataWindow\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 180\n",
    "\n",
    "# use GPU if possible\n",
    "device = pt.device(\"cuda\") if pt.cuda.is_available() else pt.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# define prediction horizon and type of dimensionality reduction\n",
    "PRED_HORIZON = 2\n",
    "DIM_REDUCTION = \"VAE\"       # one of (\"SVD\" / \"VAE\")\n",
    "N_LATENT = config.SVD_rank if DIM_REDUCTION == \"SVD\" else config.VAE_latent_size\n",
    "\n",
    "# define paths\n",
    "DATA_PATH = join(parent_dir, \"data\", \"full_pipeline_data\")\n",
    "VAE_PATH = join(parent_dir, \"output\", \"VAE\", \"latent_study\", config.VAE_model)\n",
    "SVD_PATH = join(parent_dir, \"output\", \"SVD\", \"U.pt\")\n",
    "FC_MODEL = \"32_128_1\"\n",
    "FC_PATH = join(parent_dir, \"output\", \"FC\", DIM_REDUCTION, \"param_study\", f\"pred_horizon_{PRED_HORIZON}\")\n",
    "OUTPUT_PATH = join(parent_dir, \"output\", \"FC\", DIM_REDUCTION, \"param_study\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Parameter Study Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load study results\n",
    "study_results = pt.load(join(FC_PATH, \"study_results.pt\"))\n",
    "param_combinations = list(study_results.keys())\n",
    "\n",
    "# find parameter combinations of study and extract test loss\n",
    "input_width = [int(param_set.split('_')[0]) for param_set in param_combinations]\n",
    "hidden_size = [int(param_set.split('_')[1]) for param_set in param_combinations]\n",
    "n_hidden = [int(param_set.split('_')[2]) for param_set in param_combinations]\n",
    "test_losses = [study_results[param_set][0][\"test_loss\"].values[-10:].mean() for param_set in param_combinations]\n",
    "\n",
    "# Sort the indexed losses based on the values (ascending order)\n",
    "sorted_losses = sorted(list(enumerate(test_losses)), key=lambda x: x[1])\n",
    "lowest_loss_idx = [index for index, _ in sorted_losses[:5]]\n",
    "\n",
    "print(\"The param combinations with the lowest loss: [input_width, hidden_size, n_hidden]\")\n",
    "print([param_combinations[i] for i in lowest_loss_idx]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D scatter plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(input_width, hidden_size, n_hidden, c=test_losses, cmap='viridis', s=100)\n",
    "ax.set_xlabel(\"input width\")\n",
    "ax.set_xticks([32])\n",
    "ax.set_ylabel(\"hidden layer neurons\")\n",
    "ax.set_zlabel(\"hidden layers\")\n",
    "\n",
    "cbar = plt.colorbar(ax.scatter(input_width, hidden_size, n_hidden, c=test_losses, cmap='viridis'), pad=0.15)\n",
    "cbar.set_label('Test Loss')\n",
    "fig.tight_layout()\n",
    "fig.savefig(join(OUTPUT_PATH, f\"{DIM_REDUCTION}_FC_predhor{PRED_HORIZON}_param_study.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestep computation (test datasat comprises two flow conditions, 500 timesteps each)\n",
    "TIMESTEP_1 = config.timestep_prediction                  \n",
    "TIMESTEP_2 = TIMESTEP_1 + config.time_steps_per_cond\n",
    "print(f\"Predicted timestep for each flow condition is {TIMESTEP_1} out of {config.time_steps_per_cond}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experimental test data\n",
    "test_data_orig = pt.load(join(DATA_PATH, f\"{DIM_REDUCTION}_test.pt\"))\n",
    "\n",
    "# If using SVD, unflatten the data\n",
    "if DIM_REDUCTION == \"SVD\":\n",
    "    test_data_orig = test_data_orig.unflatten(dim=0, sizes=config.target_resolution)\n",
    "\n",
    "\n",
    "# load coordinate grids\n",
    "coords = pt.load(join(Path(DATA_PATH).parent, \"coords_interp.pt\"))\n",
    "xx, yy = coords\n",
    "\n",
    "# compress dataset into reduced state either by VAE or SVD\n",
    "if DIM_REDUCTION == \"VAE\":\n",
    "    (train_red, val_red, test_red), decoder = reduce_datasets_VAE(DATA_PATH, VAE_PATH, OUTPUT_PATH, device) \n",
    "elif DIM_REDUCTION == \"SVD\":\n",
    "    (train_red, val_red, test_red), U = reduce_datasets_SVD(DATA_PATH, SVD_PATH, OUTPUT_PATH) \n",
    "else:\n",
    "    raise ValueError(\"Unknown DIM_REDUCTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add this to config\n",
    "# define model parameters\n",
    "INPUT_WIDTH, HIDDEN_SIZE, N_HIDDEN_LAYERS = [int(param) for param in FC_MODEL.split(\"_\")]\n",
    "\n",
    "# create FC model and load model state dict\n",
    "FC_model = make_FC_model(\n",
    "    latent_size=N_LATENT,\n",
    "    input_width=INPUT_WIDTH, \n",
    "    hidden_size=HIDDEN_SIZE, \n",
    "    n_hidden_layers=N_HIDDEN_LAYERS\n",
    ")\n",
    "FC_model.load(join(FC_PATH, FC_MODEL + \".pt\"))\n",
    "FC_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load scaler used during model training\n",
    "latent_scaler = pt.load(join(OUTPUT_PATH, \"scaler.pt\"))\n",
    "\n",
    "# feed reduced and scaled dataset into DataWindow class to create TimeSeriesTensorDatasets\n",
    "data_window = DataWindow(train=latent_scaler.scale(train_red), test=latent_scaler.scale(test_red), input_width=INPUT_WIDTH, pred_horizon=PRED_HORIZON)\n",
    "_, target_idx = data_window.rolling_window(test_red.shape[1])\n",
    "target_idx = target_idx.tolist()\n",
    "\n",
    "test_windows = data_window.test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize losses\n",
    "latent_loss = []\n",
    "orig_loss = []\n",
    "\n",
    "# find index of input-target pair in dataset that predicts TIMESTEP\n",
    "timestep_1_id = find_target_index_in_dataset(nested_list=target_idx, target_id=TIMESTEP_1)\n",
    "timestep_2_id = find_target_index_in_dataset(nested_list=target_idx, target_id=TIMESTEP_2)\n",
    "# FIXME fix the indexing problem\n",
    "print(timestep_1_id)\n",
    "print(timestep_2_id)\n",
    "print(target_idx)\n",
    "\n",
    "with pt.no_grad():\n",
    "    inputs, targets = test_windows[timestep_1_id]\n",
    "    # add batch dimension with unsqueeze(0)\n",
    "    inputs = inputs.flatten().unsqueeze(0).to(device)\n",
    "    targets = targets.unsqueeze(0).to(device)\n",
    "\n",
    "    for step in range(PRED_HORIZON):\n",
    "        # shift input sequence by one: add last prediction while discarding first input\n",
    "        if step != 0:\n",
    "            inputs = shift_input_sequence(orig_seq=inputs, new_pred=pred)\n",
    "\n",
    "        # time-evolution (autoregressive)\n",
    "        pred = FC_model(inputs)\n",
    "        latent_loss.append(mse_loss(targets[:, :, step], pred))\n",
    "\n",
    "        # re-scaling\n",
    "        pred_rescaled = latent_scaler.rescale(pred)\n",
    "\n",
    "        # expand to full space either by VAE or SVD\n",
    "        if DIM_REDUCTION == \"VAE\":\n",
    "            # forward pass through decoder\n",
    "            pred_orig = decoder(pred_rescaled.unsqueeze(0)).squeeze().detach() \n",
    "        else:\n",
    "            # matrix multiplication with U, followed by adding back the temporal mean\n",
    "            pred_orig = (U @ pred_rescaled.permute(1, 0) + test_data_orig.flatten(0, 1).mean(dim=1).unsqueeze(-1)).squeeze().unflatten(dim=0, sizes=config.target_resolution)\n",
    "        \n",
    "        print(pred_orig.shape)\n",
    "\n",
    "        orig_loss.append(mse_loss(test_data_orig[:, :, target_idx[timestep_1_id][step]], pred_orig))\n",
    "\n",
    "MSE = (test_data_orig[:, :, TIMESTEP_1] - pred_orig)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Latent vs. Full Space Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(1, 1, figsize=config.standard_figsize_1)\n",
    "plt.plot(range(1, PRED_HORIZON + 1), latent_loss, label=\"reduced space loss\")\n",
    "plt.plot(range(1, PRED_HORIZON + 1), orig_loss, label=\"full space loss\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"number of autoregressive predictions\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.tight_layout\n",
    "plt.savefig(join(OUTPUT_PATH, f\"{DIM_REDUCTION}_FC_predhor{PRED_HORIZON}_origvslatentloss.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Original vs. Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "vmin_cp, vmax_cp = config.plot_lims_cp\n",
    "vmin_MSE, vmax_MSE = config.plot_lims_MSE_reconstruction\n",
    "levels_cp = pt.linspace(vmin_cp, vmax_cp, 120)\n",
    "levels_MSE = pt.linspace(vmin_MSE, vmax_MSE, 120)\n",
    "\n",
    "ax1.contourf(xx, yy, test_data_orig[:, :, TIMESTEP_1], vmin=vmin_cp, vmax=vmax_cp, levels=levels_cp)\n",
    "ax2.contourf(xx, yy, pred_orig, vmin=vmin_cp, vmax=vmax_cp, levels=levels_cp)\n",
    "cont = ax3.contourf(xx, yy, MSE, vmin=vmin_MSE, vmax=vmax_MSE, levels=levels_MSE)\n",
    "\n",
    "ax1.set_title(\"Ground Truth\")\n",
    "ax2.set_title(\"CNN-VAE-FC\")\n",
    "\n",
    "fig.subplots_adjust(right=0.95)\n",
    "cax = fig.add_axes([0.99, 0.283, 0.03, 0.424])\n",
    "cbar = fig.colorbar(cont, cax=cax,label = \"Squarred Error\")\n",
    "cbar.formatter = ticker.FormatStrFormatter(f'%.{2}f')\n",
    "\n",
    "for ax in [ax1, ax2, ax3]:\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "fig.savefig(join(OUTPUT_PATH, f\"{DIM_REDUCTION}_FC_predhor{PRED_HORIZON}_timestep_reconstr.png\"), bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
