{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# include app directory into sys.path\n",
    "parent_dir = Path(os.path.abspath('')).parent\n",
    "app_dir = join(parent_dir, \"app\")\n",
    "if app_dir not in sys.path:\n",
    "      sys.path.append(app_dir)\n",
    "\n",
    "import torch as pt\n",
    "from torch.nn.functional import mse_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker \n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from flowtorch.analysis import SVD\n",
    "import numpy as np\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "import utils.config as config\n",
    "from utils.helper_funcs import load_datasets_end_to_end\n",
    "from utils.DataWindow import DataWindow_end_to_end\n",
    "from CNN_VAE.CNN_VAE import ConvDecoder, ConvEncoder\n",
    "from LSTM.LSTM_model import LSTM\n",
    "from end_to_end.CNN_VAE_LSTM import autoencoder_LSTM\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 180\n",
    "\n",
    "# use GPU if possible\n",
    "device = pt.device(\"cuda\") if pt.cuda.is_available() else pt.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# define prediction horizon and type of dimensionality reduction\n",
    "PRED_HORIZON = 3\n",
    "N_LATENT = 64\n",
    "FC_MODEL = \"1_32_64_2\"\n",
    "\n",
    "# define paths\n",
    "DATA_PATH = join(parent_dir, \"data\", \"end_to_end\")\n",
    "OUTPUT_PATH = join(parent_dir, \"output\", \"end_to_end\", \"single\")\n",
    "MODEL_PATH = join(parent_dir, \"output\", \"end_to_end\", \"single\", f\"pred_horizon_{PRED_HORIZON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define FC model parameters\n",
    "_, INPUT_WIDTH, HIDDEN_SIZE, N_STACKED_LAYERS = [int(param) for param in FC_MODEL.split(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestep and index computation, transforming to dimensionsless time\n",
    "TIMESTEP_1, TIMESTEP_2 = (INPUT_WIDTH + 9, INPUT_WIDTH + 49)\n",
    "dimless_factor = config.U_inf / (config.c_mean * config.timesteps_per_second)\n",
    "\n",
    "TIMESTEP_dimless_split = round((config.single_flow_cond_train_share * config.time_steps_per_cond) * dimless_factor, 2)\n",
    "TIMESTEP_dimless_1= round((TIMESTEP_1 + (config.single_flow_cond_train_share * config.time_steps_per_cond)) * dimless_factor, 2)\n",
    "TIMESTEP_dimless_2= round((TIMESTEP_2 + (config.single_flow_cond_train_share * config.time_steps_per_cond)) * dimless_factor, 2)\n",
    "\n",
    "# compute prediction horizons to predict timestep 1 and 2\n",
    "pred_horizon_1 = TIMESTEP_1 - INPUT_WIDTH + 1\n",
    "pred_horizon_2 = TIMESTEP_2 - INPUT_WIDTH + 1\n",
    "\n",
    "# set a prediction horizon for comparing latent and full space loss\n",
    "pred_horizon_total = int(config.time_steps_per_cond - config.single_flow_cond_train_share * config.time_steps_per_cond - INPUT_WIDTH)\n",
    "\n",
    "print(f\"Test dataset comprises timesteps {int(config.single_flow_cond_train_share * config.time_steps_per_cond)} - {config.time_steps_per_cond}.\")    \n",
    "print(f\"The end-to-end model takes the first {INPUT_WIDTH} timesteps as input.\\n\")     \n",
    "print(f\"Predicted timestep 1 (index) is:            {TIMESTEP_1}\")\n",
    "print(f\"    which equals a dimensionless time:      {TIMESTEP_dimless_1}\")\n",
    "print(f\"    and a prediction horizon of:            {pred_horizon_1}\\n\")\n",
    "print(f\"Predicted timestep 2 (index) is:            {TIMESTEP_2}\")\n",
    "print(f\"    which equals a dimensionless time:      {TIMESTEP_dimless_2}\")\n",
    "print(f\"    and a prediction horizon of:            {pred_horizon_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experimental data\n",
    "train, test = load_datasets_end_to_end(DATA_PATH)\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "# load coordinate grids\n",
    "coords = pt.load(join(Path(DATA_PATH).parent, \"coords_interp.pt\"))\n",
    "xx, yy = coords\n",
    "\n",
    "# load scalers \n",
    "E2E_scaler = pt.load(join(Path(DATA_PATH).parent, \"VAE_LSTM_scaler.pt\"))\n",
    "VAE_scaler = pt.load(join(Path(DATA_PATH).parent, \"VAE_scaler.pt\"))\n",
    "\n",
    "# load sequential model results\n",
    "orig_loss_seq = pt.load(join(OUTPUT_PATH, \"orig_loss_seq_model.pt\"))\n",
    "PDE_seq = pt.load(join(OUTPUT_PATH, \"PDE_seq.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed reduced and scaled dataset into DataWindow class to create TimeSeriesTensorDatasets\n",
    "data_window = DataWindow_end_to_end(train=train, test=test, input_width=INPUT_WIDTH, pred_horizon=pred_horizon_total)\n",
    "input_idx, target_idx = data_window.rolling_window(test.shape[2])\n",
    "target_idx = target_idx.tolist()\n",
    "\n",
    "print(f\"Input indices of first window range from:           {input_idx[0][0]} to {input_idx[0][-1]}\")\n",
    "print(f\"Target indices of first window range from:          {target_idx[0][0]} to {target_idx[0][-1]}\")\n",
    "print(f\"Number of possible windows:                            {len(input_idx)}\")\n",
    "\n",
    "test_windows = data_window.test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models\n",
    "encoder = ConvEncoder(\n",
    "    in_size=config.target_resolution,\n",
    "    n_channels=config.VAE_input_channels,\n",
    "    n_latent=N_LATENT,\n",
    "    variational=True,\n",
    "    layernorm=True\n",
    ")\n",
    "decoder = ConvDecoder(\n",
    "    in_size=config.target_resolution,\n",
    "    n_channels=config.VAE_output_channels,\n",
    "    n_latent=N_LATENT,\n",
    "    layernorm=True,\n",
    "    squash_output=True\n",
    ")\n",
    "lstm = LSTM(\n",
    "    latent_size=N_LATENT, \n",
    "    hidden_size=HIDDEN_SIZE, \n",
    "    num_layers=N_STACKED_LAYERS\n",
    "    )\n",
    "\n",
    "model = autoencoder_LSTM(encoder=encoder, LSTM=lstm, decoder=decoder)\n",
    "model.load(join(MODEL_PATH, FC_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_horizon = pred_horizon_total\n",
    "print(pred_horizon)\n",
    "\n",
    "with pt.no_grad():\n",
    "    inputs, targets = test_windows[0]\n",
    "\n",
    "    # add batch dimension with unsqueeze(0)\n",
    "    inputs = inputs.unsqueeze(0).to(device)\n",
    "    test_reconstr = model(inputs, pred_horizon).squeeze().detach()\n",
    "\n",
    "\n",
    "test_reconstr = VAE_scaler.scale(E2E_scaler.rescale(test_reconstr))\n",
    "test_original = VAE_scaler.scale(E2E_scaler.rescale(test[:,:,INPUT_WIDTH:]))\n",
    "orig_loss = [mse_loss(test_original[:, :, step], test_reconstr[:, :, step]) for step in range(pred_horizon_total)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Full Space Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = '#FFA500'\n",
    "fig = plt.subplots(1, 1, figsize=config.orig_vs_latent_loss_figsize)\n",
    "plt.plot(range(1, pred_horizon_total + 1), orig_loss, ls=\":\", label=\"full space loss E2E\", color=color)\n",
    "plt.plot(range(1, pred_horizon_total + 1), orig_loss_seq, label=\"full space loss seq.\", color=\"darkolivegreen\")\n",
    "plt.ylabel(\"Test MSE\")\n",
    "plt.xlabel(\"number of autoregressive predictions\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(config.plot_lims_orig_vs_latent_loss)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.tight_layout\n",
    "plt.savefig(join(OUTPUT_PATH, f\"E2E_single_origvslatentloss.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create animations of the reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE = (test_original - test_reconstr)**2\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(6, 2.5))\n",
    "vmin_cp, vmax_cp = config.plot_lims_cp\n",
    "vmin_MSE, vmax_MSE = config.plot_lims_MSE_reconstruction\n",
    "levels_cp = pt.linspace(vmin_cp, vmax_cp, 120)\n",
    "levels_MSE = pt.linspace(vmin_MSE, vmax_MSE, 120)\n",
    "\n",
    "def update(frame):\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    ax3.clear()\n",
    "    \n",
    "    ax1.contourf(xx, yy, test_original[:, :, frame], vmin=vmin_cp, vmax=vmax_cp, levels=levels_cp)\n",
    "    ax2.contourf(xx, yy, test_reconstr[:, :, frame], vmin=vmin_cp, vmax=vmax_cp, levels=levels_cp)\n",
    "    cont = ax3.contourf(xx, yy, SE[:, :, frame], vmin=vmin_MSE, vmax=vmax_MSE, levels=levels_MSE)\n",
    "\n",
    "    ax1.set_title(\"Ground Truth\")\n",
    "    ax2.set_title(\"E2E CNN-VAE-LSTM\")\n",
    "\n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([])\n",
    "\n",
    "ani = animation.FuncAnimation(fig, update, frames=SE.shape[2], interval=100)\n",
    "ani.save(join(OUTPUT_PATH, f\"E2E_reconstruction.gif\"), writer='pillow')\n",
    "plt.close(fig)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Power Spectra of POD Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten original and reconstructed test dataset\n",
    "test_original = test_original.flatten(0,1)\n",
    "test_reconstr = test_reconstr.flatten(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_original= SVD(test_original - test_original.mean(dim=1).unsqueeze(-1), rank=1e5)\n",
    "V_original = svd_original.V\n",
    "\n",
    "svd_reconstr = SVD(test_reconstr - test_reconstr.mean(dim=1).unsqueeze(-1), rank=1e5)\n",
    "V_reconstr = svd_reconstr.V\n",
    "\n",
    "N = test_original.shape[1]\n",
    "num_modes = 6\n",
    "sample_rate = 2000          # [Hz]\n",
    "y_lims = config.plot_lims_power_spectra_single\n",
    "psd_mse = []\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=config.power_sepctra_figsize, sharex=True)\n",
    "for row in range(3):\n",
    "    for col in range(2):\n",
    "        # Calculate the mode index and retrieve mode coefficients\n",
    "        mode = row * 2 + col                   \n",
    "        original_mode_coeffs = V_original[:, mode].numpy()\n",
    "        reconstr_mode_coeffs = V_reconstr[:, mode].numpy()\n",
    "\n",
    "        # Compute FFT and PSD\n",
    "        original_fft = fft(original_mode_coeffs)\n",
    "        original_psd = np.abs(original_fft)**2 / len(original_fft)\n",
    "        reconstr_fft = fft(reconstr_mode_coeffs)\n",
    "        reconstr_psd = np.abs(reconstr_fft)**2 / len(reconstr_fft)\n",
    "\n",
    "        psd_mse.append(mse_loss(pt.from_numpy(original_psd), pt.from_numpy(reconstr_psd)))\n",
    "\n",
    "        # Frequency values for plotting\n",
    "        freq = fftfreq(len(original_mode_coeffs), d=1/sample_rate)* config.c_mean  / config.U_inf\n",
    "\n",
    "        # Use only the positive frequencies (discard negative frequency half)\n",
    "        freq = freq[:len(freq)//2]\n",
    "        original_psd = original_psd[:len(original_psd)//2]\n",
    "        reconstr_psd = reconstr_psd[:len(reconstr_psd)//2]\n",
    "\n",
    "        # Plot the power spectra\n",
    "        ax[row, col].semilogy(freq, original_psd, linewidth=0.5, color=\"black\", label=\"Experimental Data\")\n",
    "        ax[row, col].semilogy(freq, PDE_seq[mode], linewidth=0.7, color=\"cornflowerblue\", linestyle='dashed', label=\"seq. CNN-VAE-LSTM\")\n",
    "        ax[row, col].semilogy(freq, reconstr_psd, linewidth=1.2, color=color, linestyle='dotted', label=\"E2E  CNN-VAE-LSTM\")\n",
    "        ax[row, col].set_title(f\"Mode Coefficient {mode + 1}\")\n",
    "        ax[row, col].grid()\n",
    "        ax[row, col].set_yticklabels([])\n",
    "        ax[row, col].set_yticks([])\n",
    "        ax[row, col].set_ylim(y_lims)\n",
    "\n",
    "        \n",
    "ax[2, 0].set_xlabel(rf\"Strouhal number $St$\")\n",
    "ax[2, 1].set_xlabel(rf\"Strouhal number $St$\")\n",
    "handles, labels = ax[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"lower center\", bbox_to_anchor=(0.5, -0.05), ncol=3) \n",
    "\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "# Adjust spacing between subplots to accommodate the legend\n",
    "plt.subplots_adjust(bottom=0.5)\n",
    "fig.tight_layout()\n",
    "fig.savefig(join(OUTPUT_PATH, f\"E2E_single_power_spectra.png\"), bbox_inches=\"tight\")\n",
    "\n",
    "print(\"MSE is:                  \", sum(psd_mse) / len(psd_mse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
