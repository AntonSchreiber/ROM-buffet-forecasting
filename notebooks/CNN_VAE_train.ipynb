{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN VAE Training with varying latent size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os.path import join\n",
    "parent_dir = os.path.abspath(join(os.getcwd(), os.pardir))\n",
    "app_dir = join(parent_dir, \"app\")\n",
    "if app_dir not in sys.path:\n",
    "      sys.path.append(app_dir)\n",
    "\n",
    "from pathlib import Path\n",
    "import torch as pt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from CNN_VAE import ConvDecoder, ConvEncoder, Autoencoder\n",
    "from utils.training_loop import train_cnn_vae\n",
    "import utils.config as config\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "importlib.reload(config)\n",
    "\n",
    "pt.manual_seed(0)\n",
    "plt.rcParams[\"figure.dpi\"] = 180\n",
    "\n",
    "# use GPU if possible\n",
    "device = pt.device(\"cuda:0\") if pt.cuda.is_available() else pt.device(\"cpu\")\n",
    "\n",
    "DATA_PATH = Path(os.path.abspath('')).parent / \"data\"\n",
    "OUTPUT_PATH = Path(os.path.abspath('')).parent / \"output\" / \"VAE\" / \"latent_study\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_sizes = list(range(0, 301, 100))\n",
    "print(latent_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_dataset = pt.load(join(DATA_PATH, \"train_dataset.pt\"))\n",
    "val_dataset = pt.load(join(DATA_PATH, \"val_dataset.pt\"))\n",
    "test_dataset = pt.load(join(DATA_PATH, \"test_dataset.pt\"))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create VAE model\n",
    "def make_VAE_model(n_latent: int = 256) -> pt.nn.Module:\n",
    "    encoder = ConvEncoder(\n",
    "        in_size=config.target_resolution,\n",
    "        n_channels=config.input_channels,\n",
    "        n_latent=config.latent_size,\n",
    "        variational=True,\n",
    "        layernorm=True\n",
    "    )\n",
    "\n",
    "    decoder = ConvDecoder(\n",
    "        in_size=config.target_resolution,\n",
    "        n_channels=config.output_channels,\n",
    "        n_latent=config.latent_size,\n",
    "        layernorm=True,\n",
    "        squash_output=True\n",
    "    )\n",
    "\n",
    "    autoencoder = Autoencoder(encoder, decoder)\n",
    "    autoencoder.to(device)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start study\n",
    "print(\"Running study...\")\n",
    "results = []\n",
    "for latent_size in latent_sizes:\n",
    "    print(\"Training autoencoder with {} bottleneck neurons ...\".format(latent_size))\n",
    "    model = make_VAE_model(latent_size)\n",
    "    optimizer = pt.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    scheduler = pt.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode=\"min\", factor=0.2)\n",
    "\n",
    "    results.append(train_cnn_vae(\n",
    "        model=model,\n",
    "        loss_func=nn.MSELoss(),\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=config.epochs,\n",
    "        optimizer=optimizer,\n",
    "        lr_schedule=scheduler,\n",
    "    ))\n",
    "    # create directory to save model state\n",
    "    subfolder = \"../output/VAE/latent_study/\" + str(latent_size)\n",
    "    !mkdir -p $subfolder\n",
    "    model.save(str(join(OUTPUT_PATH, str(latent_size), str(latent_size))))\n",
    "    print(\"\\n\")\n",
    "pt.save(results, join(OUTPUT_PATH, \"training_results.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pt.load(join(OUTPUT_PATH, \"training_results.pt\"))\n",
    "# plot study results, this will be in the \"Training the Autoencoder\" chapter\n",
    "for i, latent_size in enumerate(latent_sizes):\n",
    "    plt.plot(results[i][\"epoch\"], results[i][\"val_loss\"], lw=1, label=\"{} bottleneck neurons\".format(latent_size))\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xlim(0, config.epochs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(join(Path(OUTPUT_PATH).parent, \"Val_loss_results.png\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
